<div align="center">

[< *Architecture*](1.2.architecture.md) | [**Summary**](0.0.index.md) | [*Introduction* >](2.1.introduction.md)

## Documentation

### I.3. Platform - Pipeline

</div>

<div align="center">

[Flowchart](#i31-ci--cd-pipeline-flowchart) | [Overview](#i32-pipeline-overview) | [Key Objectives](#i33-key-objectives) | [Process](#i34-pipeline-process)
</div>

### I.3.1. CI / CD Pipeline Flowchart

A CI/CD pipeline constitutes an automated sequence that builds, tests, and deploys applications in a consistent and reproducible manner.

This pipeline emphasizes **security, reproducibility, and consistency** over full automation. Each step — from build to deployment — is designed to ensure traceability, controlled promotion, and reproducible environments rather than prioritizing continuous delivery speed.

> Traceability: "Each step is traceable, meaning it is possible to identify which code version, which test, and which artifact led to a given deployment."

The following diagram illustrates the global CI/CD pipeline. Each numbered step represents a major phase in the delivery process, referenced throughout the documentation for consistency.

<div align="center">

![ CI / CD Pipeline Flowchart](../assets/ci-cd-pipeline.svg)
</div>

> **Note:** This flowchart represents the global pipeline overview. Not all subsystems are shown here.

### I.3.2. Pipeline Overview

The subsequent section provides a detailed description of each phase of the pipeline, following the order presented in the diagram above.

1. **Development:** Developers build and experiment freely with their components.
2. **Staging:** Stable versions undergo testing to verify correct functionality before sharing.
3. **Registry:** Approved versions are stored securely in an internal library.
4. **Integration:** All services are combined and tested to ensure full compatibility.
5. **Promotion:** Successful builds are marked as ready for wider testing.
6. **Pre-Production:** The platform is tested on an internal machine simulating real client use.
7. **Client Repository:** Each client maintains its own copy of platform settings and updates.
8. **Production:** The client’s system runs the final, verified platform version within a secure local environment.

### I.3.3. Key Objectives

- Ensure consistent, traceable builds across all environments.  
- Validate component compatibility before release.  
- Guarantee that client deployments remain stable, secure, and reproducible.

### I.3.4. Pipeline Process

#### 1. Dev Step

Within the *main* branch of each component, development proceeds without restrictions imposed by the pipeline’s constraints. This environment is intentionally flexible, allowing developers to push changes freely without triggering the full automation or validation workflow. It functions as a **rule-free sandbox**, facilitating rapid iteration and experimentation.

#### 2. Staging Step

The second stage of a component’s development cycle occurs within the *staging* branch. Once the *main* branch reaches a stable state and is prepared for integration into the platform, its changes are merged or pushed into *staging*.

> When adhering to a linear workflow, no merge is required between the main and staging branches; commits may be pushed directly to staging once validated.

This action triggers a GitLab CI pipeline, which executes the `stage-unit-test`￼ rule defined in the [**root Makefile**](./1.2.architecture.md#component-makefile). This rule validates the component’s integrity within the platform context.

- In case of failure, the process returns to Step 1 for issue resolution.
- Upon success:

    - the image is built during the build stage,
    - an SBOM (Software Bill of Materials) is generated and attached,
    - the image is signed with Cosign,
    - and the image is subsequently pushed to the internal registry.

#### 3. Registry

A single internal registry is maintained for all components. Each new image pushed from the staging branch is:

- tagged as `:staged`,
- versioned according to the [versioning convention]() (to be linked),
- and referenced by its immutable sha256 digest.

This approach ensures that every build is uniquely identifiable, verifiably signed, and traceable back to its source commit and pipeline execution.

> The registry functions as an internal library where container images are stored — packaged versions of components ready for deployment.

#### 4. Platform Integration Hub

This step involves assembling all services to verify their correct interoperability.

Once component images are validated and pushed to the internal registry, the Platform Integration Hub serves as the **central assembly point** where all services are combined and verified within a unified environment.

Its role is to **ensure cross-component consistency** by validating that updated versions of each module remain compatible within the full platform stack.

Each staged image is referenced in the Integration Hub’s Docker Compose configuration, replicating the production topology **using the same OCI images**. This environment is utilized to perform integration tests covering service interoperability, database migrations, and API contracts across components.

> In case of failure, the process returns to Step 1 for issue resolution.

#### 5. Registry (Candidate Promotion)

Following successful integration, the pipeline promotes each validated image from the `:staged` tag to a `:candidate` tag. This operation does not involve rebuilding the image; rather, it retags the same digest to designate it as a release candidate.

The `:candidate` tag identifies a frozen version of all platform components, **prepared for deployment in pre-production**.

> Promotion is fully traceable: each candidate version retains the same SBOM, Cosign signature, and provenance as its originating staged image.

#### 6. Internal NixOS Deployment (Pre-Production)

This step pertains to a **dedicated repository linked to an internal NixOS machine**, which functions as the **pre-production environment** for the platform, facilitating testing prior to client deployment.

This repository contains the **flake configuration** used by the NixOS system to deploy the platform. The flakes reference component images tagged as `:candidate`, ensuring that the exact artifacts validated during integration are employed in deployment.

This machine serves as a **long-term validation environment** for executing various categories of tests. Upon successful completion of smoke, long-term, and performance tests, the candidate images may be promoted to `:latest`, thereby marking the new stable platform version.

> In case of failure, the process returns to Step 4 to incorporate tests capable of detecting the issues, followed by a return to Step 1 for resolution.

#### 7. Client Repository

Each client environment maintains a **dedicated repository** containing the **flake configuration** used by the NixOS client system.

This repository tracks:

- the platform version currently deployed for the client,
- optional custom modules or integrations,
- and secrets or environment overrides specific to that deployment.

Each flake may reference component images **tagged as** `:latest` for LTS clients, or alternatively reference **a locked version to ensure stability** where required.

#### 8. Client Machine (Production)

Each client production machine operates its own flake configuration **pulled from the associated repository**.

This machine serves as the final stage of the delivery chain, providing a **fully reproducible and signed deployment** of the platform for the client. Regular smoke and health checks are performed to confirm the stability of production services and to verify that all signatures, SBOMs, and runtime environments remain trusted and compliant.

Updates are applied through two methods:
- **Automatic pull:** the NixOS system periodically rebuilds from the client repository to retrieve the latest flake configuration.  
- **Manual update:** to maintain security and GDPR compliance, administrators may trigger updates manually within the client’s local network using a temporary connection or a shared copy of the repository.

<div align="center">

#
[< *Architecture*](1.2.architecture.md) | [**Summary**](0.0.index.md) | [*Introduction* >](2.1.introduction.md)